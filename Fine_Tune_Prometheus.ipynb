{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e012a763",
   "metadata": {},
   "source": [
    "# Prometheus Prompt Enhancement - LoRA Fine-Tuning\n",
    "\n",
    "This notebook fine-tunes Mistral-7B using LoRA on the Prometheus training dataset to enhance prompts with model-specific styles (ChatGPT, Claude, Gemini).\n",
    "\n",
    "**Dataset**: 1,000 training examples with model-specific enhancements  \n",
    "**Base Model**: Mistral-7B-Instruct-v0.1  \n",
    "**Method**: QLoRA (4-bit quantization + LoRA)  \n",
    "**Expected Time**: ~2 hours on T4 GPU\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "Before running:\n",
    "1. Enable GPU: **Runtime** ‚Üí **Change runtime type** ‚Üí **T4 GPU**\n",
    "2. Upload `training_dataset.jsonl` to Google Drive at: `/MyDrive/Prometheus/training_data/`\n",
    "3. Run cells sequentially from top to bottom\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6cf377",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Install compatible packages for Google Colab's current environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076fc5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"üîß Prometheus Fine-Tuning Environment Setup\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Uninstall ALL conflicting packages\n",
    "print(\"\\nüóëÔ∏è  Step 1: Removing old packages...\\n\")\n",
    "packages_to_remove = [\n",
    "    \"bitsandbytes\", \"transformers\", \"peft\", \"accelerate\", \n",
    "    \"datasets\", \"trl\", \"triton\", \"torch\"\n",
    "]\n",
    "\n",
    "for pkg in packages_to_remove:\n",
    "    subprocess.run(\n",
    "        [sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"-q\", pkg],\n",
    "        capture_output=True\n",
    "    )\n",
    "    print(f\"  ‚úì Removed {pkg}\")\n",
    "\n",
    "print(\"\\nüì¶ Step 2: Installing working package set...\\n\")\n",
    "\n",
    "# Install PyTorch with CUDA 12.1 support\n",
    "print(\"  ‚Üí Installing PyTorch 2.5.1+cu121...\")\n",
    "subprocess.check_call([\n",
    "    sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
    "    \"torch==2.5.1\", \n",
    "    \"torchvision==0.20.1\", \n",
    "    \"torchaudio==2.5.1\",\n",
    "    \"--index-url\", \"https://download.pytorch.org/whl/cu121\"\n",
    "])\n",
    "print(\"  ‚úì PyTorch 2.5.1+cu121\\n\")\n",
    "\n",
    "# Install compatible ML packages\n",
    "ml_packages = [\n",
    "    \"transformers==4.46.0\",\n",
    "    \"peft==0.13.2\",\n",
    "    \"bitsandbytes==0.44.1\",\n",
    "    \"accelerate==1.1.1\",\n",
    "    \"datasets==3.1.0\",\n",
    "    \"trl==0.12.1\",\n",
    "    \"scipy\",\n",
    "    \"sentencepiece\",\n",
    "    \"protobuf\"\n",
    "]\n",
    "\n",
    "for pkg in ml_packages:\n",
    "    try:\n",
    "        print(f\"  ‚Üí Installing {pkg}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
    "        print(f\"  ‚úì {pkg}\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"  ‚úó Failed: {pkg}\")\n",
    "        # Try without version constraint\n",
    "        base_pkg = pkg.split(\"==\")[0]\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", base_pkg])\n",
    "            print(f\"  ‚úì {base_pkg} (fallback)\")\n",
    "        except:\n",
    "            print(f\"  ‚úó Could not install {base_pkg}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Installation complete!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚ö†Ô∏è  CRITICAL: RESTART RUNTIME NOW!\")\n",
    "print(\"\\nüìã Next Steps:\")\n",
    "print(\"  1. Runtime ‚Üí Restart runtime\")\n",
    "print(\"  2. Run Cell 2 (Mount Drive & Verify)\")\n",
    "print(\"  3. Continue from Cell 3 onwards\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07777dd4",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive & Verify Environment\n",
    "\n",
    "**‚ö†Ô∏è Run this after restarting runtime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"üîå Mounting Google Drive...\\n\")\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    print(\"‚úÖ Google Drive mounted successfully\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to mount Google Drive: {e}\")\n",
    "    print(\"   Please run: drive.mount('/content/drive', force_remount=True)\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Import required packages\n",
    "print(\"üìö Importing packages...\\n\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForCausalLM,\n",
    "        BitsAndBytesConfig,\n",
    "        TrainingArguments,\n",
    "        Trainer\n",
    "    )\n",
    "    from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "    from datasets import load_dataset\n",
    "    import json\n",
    "    \n",
    "    print(\"‚úÖ All packages imported successfully\\n\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"\\nüîÑ Solution: Restart runtime and re-run from Section 2\")\n",
    "    print(\"   (Runtime ‚Üí Restart runtime)\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Verify GPU availability\n",
    "print(\"üñ•Ô∏è  System Information:\")\n",
    "print(f\"  Python version: {sys.version.split()[0]}\")\n",
    "print(f\"  PyTorch version: {torch.__version__}\")\n",
    "print(f\"  CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"  Total VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    print(\"\\n‚úÖ GPU is ready for training\\n\")\n",
    "else:\n",
    "    print(\"\\n‚ùå GPU not available!\")\n",
    "    print(\"   Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU ‚Üí Save\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df27e2e",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Set paths and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf7f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PATHS (Adjust if needed) =====\n",
    "DATASET_PATH = \"/content/drive/MyDrive/Prometheus/training_data/training_dataset.jsonl\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/Prometheus/models/prometheus-lora\"\n",
    "CHECKPOINT_DIR = \"/content/drive/MyDrive/Prometheus/checkpoints\"\n",
    "\n",
    "# ===== MODEL CONFIGURATION =====\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# ===== LORA CONFIGURATION =====\n",
    "LORA_R = 16  # Rank (higher = more capacity but slower)\n",
    "LORA_ALPHA = 32  # Scaling factor (typically 2√ó rank)\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\n",
    "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "    \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "]\n",
    "\n",
    "# ===== TRAINING HYPERPARAMETERS =====\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 16\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 512\n",
    "WARMUP_STEPS = 100\n",
    "\n",
    "# Verify paths exist\n",
    "print(\"‚úÖ Configuration loaded\\n\")\n",
    "print(f\"üìä Training Configuration:\")\n",
    "print(f\"  Base Model: {MODEL_NAME}\")\n",
    "print(f\"  LoRA Rank: {LORA_R}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Effective Batch: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Learning Rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Max Sequence Length: {MAX_SEQ_LENGTH}\")\n",
    "\n",
    "# Check if dataset exists\n",
    "if not os.path.exists(DATASET_PATH):\n",
    "    print(f\"\\n‚ùå Dataset not found: {DATASET_PATH}\")\n",
    "    print(\"\\nüì§ Please upload your training_dataset.jsonl to:\")\n",
    "    print(f\"   {os.path.dirname(DATASET_PATH)}/\")\n",
    "    print(\"\\n   In Google Drive web interface, create the folder structure if needed.\")\n",
    "else:\n",
    "    file_size = os.path.getsize(DATASET_PATH) / (1024 * 1024)\n",
    "    print(f\"\\n‚úÖ Dataset found: {file_size:.2f} MB\")\n",
    "\n",
    "# Create output directories\n",
    "for directory in [OUTPUT_DIR, CHECKPOINT_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"‚úÖ Directory ready: {directory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2609ea78",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Dataset\n",
    "\n",
    "Load the Prometheus training dataset with error handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda69fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìÇ Loading dataset...\\n\")\n",
    "\n",
    "try:\n",
    "    # Load dataset\n",
    "    dataset = load_dataset('json', data_files=DATASET_PATH, split='train')\n",
    "    print(f\"‚úÖ Loaded {len(dataset)} examples\\n\")\n",
    "    \n",
    "    # Verify dataset structure\n",
    "    required_fields = ['input_prompt', 'enhanced_prompt', 'target_model']\n",
    "    sample = dataset[0]\n",
    "    \n",
    "    missing_fields = [field for field in required_fields if field not in sample]\n",
    "    if missing_fields:\n",
    "        raise ValueError(f\"Dataset missing required fields: {missing_fields}\")\n",
    "    \n",
    "    print(\"‚úÖ Dataset structure validated\\n\")\n",
    "    print(\"üìã Dataset fields:\", list(sample.keys()))\n",
    "    \n",
    "    # Split into train/validation (90/10)\n",
    "    dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = dataset['train']\n",
    "    eval_dataset = dataset['test']\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Split:\")\n",
    "    print(f\"  Training examples: {len(train_dataset)}\")\n",
    "    print(f\"  Validation examples: {len(eval_dataset)}\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(f\"\\nüìù Sample entry:\")\n",
    "    print(json.dumps({\n",
    "        'input_prompt': train_dataset[0]['input_prompt'][:100] + '...',\n",
    "        'enhanced_prompt': train_dataset[0]['enhanced_prompt'][:100] + '...',\n",
    "        'target_model': train_dataset[0]['target_model']\n",
    "    }, indent=2))\n",
    "    \n",
    "    # Count target model distribution\n",
    "    from collections import Counter\n",
    "    model_counts = Counter(train_dataset['target_model'])\n",
    "    print(f\"\\nüìä Target Model Distribution:\")\n",
    "    for model, count in sorted(model_counts.items()):\n",
    "        print(f\"  {model}: {count} examples ({count/len(train_dataset)*100:.1f}%)\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Dataset file not found: {DATASET_PATH}\")\n",
    "    print(\"\\n   Please upload training_dataset.jsonl to Google Drive\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    print(f\"\\n   Error type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8f71a7",
   "metadata": {},
   "source": [
    "## 5. Format Data for Instruction Tuning\n",
    "\n",
    "Convert dataset to Mistral's instruction format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469729b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîÑ Formatting dataset for instruction tuning...\\n\")\n",
    "\n",
    "def format_instruction(example):\n",
    "    \"\"\"\n",
    "    Format training example as Mistral instruction-following conversation.\n",
    "    \n",
    "    Handles missing fields gracefully with defaults.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        target_model = example.get('target_model', 'ChatGPT')\n",
    "        input_prompt = example.get('input_prompt', '')\n",
    "        enhanced_prompt = example.get('enhanced_prompt', '')\n",
    "        \n",
    "        if not input_prompt or not enhanced_prompt:\n",
    "            # Skip examples with missing prompts\n",
    "            return None\n",
    "        \n",
    "        # Mistral instruction template\n",
    "        instruction = f\"\"\"<s>[INST] You are Prometheus, an AI assistant specialized in enhancing prompts for {target_model}.\n",
    "\n",
    "Given a user's initial prompt, enhance it following {target_model}'s best practices while preserving the original intent.\n",
    "\n",
    "User's prompt: {input_prompt} [/INST]\n",
    "\n",
    "{enhanced_prompt}</s>\"\"\"\n",
    "        \n",
    "        return {\"text\": instruction}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error formatting example: {e}\")\n",
    "        return None\n",
    "\n",
    "try:\n",
    "    # Apply formatting\n",
    "    train_dataset = train_dataset.map(format_instruction, remove_columns=train_dataset.column_names)\n",
    "    eval_dataset = eval_dataset.map(format_instruction, remove_columns=eval_dataset.column_names)\n",
    "    \n",
    "    # Filter out None values (failed formatting)\n",
    "    train_dataset = train_dataset.filter(lambda x: x['text'] is not None)\n",
    "    eval_dataset = eval_dataset.filter(lambda x: x['text'] is not None)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset formatting complete\")\n",
    "    print(f\"  Training examples: {len(train_dataset)}\")\n",
    "    print(f\"  Validation examples: {len(eval_dataset)}\")\n",
    "    \n",
    "    # Show formatted sample\n",
    "    print(f\"\\nüìù Sample formatted instruction (first 600 chars):\")\n",
    "    print(train_dataset[0]['text'][:600] + \"...\\n\")\n",
    "    \n",
    "    # Check average length\n",
    "    avg_length = sum(len(ex['text']) for ex in train_dataset) / len(train_dataset)\n",
    "    print(f\"üìè Average instruction length: {avg_length:.0f} characters\")\n",
    "    \n",
    "    if avg_length > MAX_SEQ_LENGTH * 4:  # Rough estimate (1 token ‚âà 4 chars)\n",
    "        print(f\"\\n‚ö†Ô∏è  Warning: Average length may exceed MAX_SEQ_LENGTH={MAX_SEQ_LENGTH}\")\n",
    "        print(f\"   Consider increasing MAX_SEQ_LENGTH or truncating prompts\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error formatting dataset: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55ef0c3",
   "metadata": {},
   "source": [
    "## 6. Load Model with 4-bit Quantization\n",
    "\n",
    "Load Mistral-7B with QLoRA configuration. **This takes 2-3 minutes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c3fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Loading Mistral-7B model with 4-bit quantization...\\n\")\n",
    "print(\"‚è≥ This will take 2-3 minutes to download (~4.5GB)...\\n\")\n",
    "\n",
    "try:\n",
    "    # Configure 4-bit quantization\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Quantization config created\\n\")\n",
    "    \n",
    "    # Load base model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        use_cache=False  # Required for gradient checkpointing\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Model loaded successfully\\n\")\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"‚úÖ Gradient checkpointing enabled\\n\")\n",
    "    \n",
    "    # Prepare for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    print(\"‚úÖ Model prepared for QLoRA training\\n\")\n",
    "    \n",
    "    # Show model info\n",
    "    print(\"üìä Model Information:\")\n",
    "    print(f\"  Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "    print(f\"  Device map: {model.hf_device_map}\")\n",
    "    \n",
    "    # Check if model fits in VRAM\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        print(f\"\\nüíæ GPU Memory:\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  Total: {total:.2f} GB\")\n",
    "        print(f\"  Free: {total - reserved:.2f} GB\")\n",
    "        \n",
    "        if reserved > total * 0.9:\n",
    "            print(f\"\\n‚ö†Ô∏è  Warning: GPU memory usage is high ({reserved/total*100:.1f}%)\")\n",
    "            print(f\"   Training may fail. Consider reducing BATCH_SIZE.\")\n",
    "    \n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    print(\"\\n‚ùå GPU Out of Memory!\")\n",
    "    print(\"\\nüîß Solutions:\")\n",
    "    print(\"   1. Restart runtime to clear GPU memory\")\n",
    "    print(\"   2. Reduce BATCH_SIZE to 2 in Section 3\")\n",
    "    print(\"   3. Reduce MAX_SEQ_LENGTH to 384 in Section 3\")\n",
    "    print(\"   4. Use 8-bit quantization instead (load_in_8bit=True)\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading model: {e}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a2ca30",
   "metadata": {},
   "source": [
    "## 7. Configure LoRA Adapters\n",
    "\n",
    "Add LoRA adapters for parameter-efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630a0f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîß Configuring LoRA adapters...\\n\")\n",
    "\n",
    "try:\n",
    "    # Create LoRA configuration\n",
    "    lora_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        target_modules=TARGET_MODULES,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ LoRA config created\\n\")\n",
    "    \n",
    "    # Add LoRA adapters to model\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    \n",
    "    print(\"‚úÖ LoRA adapters added to model\\n\")\n",
    "    \n",
    "    # Print trainable parameters\n",
    "    print(\"üìä Model Parameters:\")\n",
    "    model.print_trainable_parameters()\n",
    "    \n",
    "    # Calculate efficiency\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_pct = 100 * trainable_params / all_params\n",
    "    \n",
    "    print(f\"\\nüí° Training Efficiency:\")\n",
    "    print(f\"  Trainable: {trainable_params:,} parameters ({trainable_pct:.2f}%)\")\n",
    "    print(f\"  Frozen: {all_params - trainable_params:,} parameters ({100-trainable_pct:.2f}%)\")\n",
    "    print(f\"\\n  This means only {trainable_pct:.2f}% of parameters will be updated during training!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error configuring LoRA: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4641f89",
   "metadata": {},
   "source": [
    "## 8. Load Tokenizer and Tokenize Dataset\n",
    "\n",
    "Prepare data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d793b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üî§ Loading tokenizer and tokenizing dataset...\\n\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    print(\"‚úÖ Tokenizer loaded\\n\")\n",
    "    \n",
    "    # Tokenization function\n",
    "    def tokenize_function(examples):\n",
    "        \"\"\"Tokenize examples with error handling.\"\"\"\n",
    "        try:\n",
    "            outputs = tokenizer(\n",
    "                examples[\"text\"],\n",
    "                truncation=True,\n",
    "                max_length=MAX_SEQ_LENGTH,\n",
    "                padding=\"max_length\",\n",
    "                return_tensors=None  # Don't convert to tensors yet\n",
    "            )\n",
    "            outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "            return outputs\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Tokenization error: {e}\")\n",
    "            return {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "    \n",
    "    # Tokenize datasets\n",
    "    print(\"‚è≥ Tokenizing training set...\")\n",
    "    tokenized_train = train_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "        desc=\"Tokenizing train\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚è≥ Tokenizing validation set...\")\n",
    "    tokenized_eval = eval_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=eval_dataset.column_names,\n",
    "        desc=\"Tokenizing eval\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Tokenization complete!\\n\")\n",
    "    \n",
    "    print(\"üìä Tokenized Dataset:\")\n",
    "    print(f\"  Training examples: {len(tokenized_train)}\")\n",
    "    print(f\"  Validation examples: {len(tokenized_eval)}\")\n",
    "    \n",
    "    # Calculate token statistics\n",
    "    sample_lengths = [len([t for t in ex['input_ids'] if t != tokenizer.pad_token_id]) \n",
    "                     for ex in tokenized_train.select(range(min(100, len(tokenized_train))))]\n",
    "    avg_tokens = sum(sample_lengths) / len(sample_lengths)\n",
    "    max_tokens = max(sample_lengths)\n",
    "    \n",
    "    print(f\"\\nüìè Token Statistics (sample of {len(sample_lengths)}):\")\n",
    "    print(f\"  Average tokens: {avg_tokens:.0f}\")\n",
    "    print(f\"  Max tokens: {max_tokens}\")\n",
    "    print(f\"  Max allowed: {MAX_SEQ_LENGTH}\")\n",
    "    \n",
    "    if max_tokens >= MAX_SEQ_LENGTH:\n",
    "        print(f\"\\n‚ö†Ô∏è  Some examples were truncated to {MAX_SEQ_LENGTH} tokens\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during tokenization: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e26a1d",
   "metadata": {},
   "source": [
    "## 9. Configure Training Arguments\n",
    "\n",
    "Set up training configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b35a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚öôÔ∏è  Configuring training arguments...\\n\")\n",
    "\n",
    "try:\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=CHECKPOINT_DIR,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        warmup_steps=WARMUP_STEPS,\n",
    "        logging_steps=10,\n",
    "        eval_steps=50,\n",
    "        save_steps=100,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_loss\",\n",
    "        greater_is_better=False,\n",
    "        fp16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        report_to=\"none\",\n",
    "        save_total_limit=3,\n",
    "        push_to_hub=False,\n",
    "        dataloader_num_workers=2,\n",
    "        remove_unused_columns=False,\n",
    "        label_names=[\"labels\"]\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Training arguments configured\\n\")\n",
    "    \n",
    "    # Calculate training steps\n",
    "    steps_per_epoch = len(tokenized_train) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS)\n",
    "    total_steps = steps_per_epoch * NUM_EPOCHS\n",
    "    \n",
    "    print(\"üìä Training Plan:\")\n",
    "    print(f\"  Total epochs: {NUM_EPOCHS}\")\n",
    "    print(f\"  Steps per epoch: {steps_per_epoch}\")\n",
    "    print(f\"  Total training steps: {total_steps}\")\n",
    "    print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "    print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "    print(f\"  Warmup steps: {WARMUP_STEPS}\")\n",
    "    \n",
    "    # Estimate time\n",
    "    # Rough estimate: 3-5 seconds per step on T4\n",
    "    estimated_seconds = total_steps * 4  # Conservative estimate\n",
    "    estimated_minutes = estimated_seconds / 60\n",
    "    \n",
    "    print(f\"\\n‚è±Ô∏è  Estimated training time: {estimated_minutes:.0f} minutes ({estimated_minutes/60:.1f} hours)\")\n",
    "    print(f\"   (Based on ~4 seconds per step on T4 GPU)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error configuring training: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821e250a",
   "metadata": {},
   "source": [
    "## 10. Train the Model\n",
    "\n",
    "**‚è∞ This will take 1-2 hours on T4 GPU**\n",
    "\n",
    "‚ö†Ô∏è **Do not close this tab or Colab will stop training!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aef9c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"üöÄ Starting training...\\n\")\n",
    "print(f\"‚è∞ Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"\\n‚ö†Ô∏è  IMPORTANT: Keep this tab open during training!\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_eval\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Trainer initialized\\n\")\n",
    "    \n",
    "    # Start training\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"\\n‚úÖ Training complete!\\n\")\n",
    "    print(f\"‚è∞ Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"‚è±Ô∏è  Total training time: {training_time/60:.1f} minutes ({training_time/3600:.2f} hours)\")\n",
    "    \n",
    "    # Show training metrics\n",
    "    print(f\"\\nüìä Final Training Metrics:\")\n",
    "    print(f\"  Final loss: {train_result.training_loss:.4f}\")\n",
    "    print(f\"  Total steps: {train_result.global_step}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\n‚ö†Ô∏è  Training interrupted by user\")\n",
    "    print(\"   Checkpoints have been saved to:\", CHECKPOINT_DIR)\n",
    "    print(\"   You can resume training from the last checkpoint\")\n",
    "    \n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    print(\"\\n‚ùå GPU Out of Memory during training!\")\n",
    "    print(\"\\nüîß Solutions:\")\n",
    "    print(\"   1. Restart runtime\")\n",
    "    print(\"   2. Reduce BATCH_SIZE to 2 in Section 3\")\n",
    "    print(\"   3. Increase GRADIENT_ACCUMULATION_STEPS to 8 (keeps effective batch size)\")\n",
    "    print(\"   4. Reduce MAX_SEQ_LENGTH to 384\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Training error: {e}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    print(f\"\\nüíæ Checkpoints saved to: {CHECKPOINT_DIR}\")\n",
    "    print(\"   You may be able to resume from the last checkpoint\")\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ff6d8d",
   "metadata": {},
   "source": [
    "## 11. Save the Model\n",
    "\n",
    "Save LoRA adapters to Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e5dbd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üíæ Saving model...\\n\")\n",
    "\n",
    "try:\n",
    "    # Save LoRA adapters\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"‚úÖ Model saved to: {OUTPUT_DIR}\\n\")\n",
    "    \n",
    "    # Save tokenizer\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    print(f\"‚úÖ Tokenizer saved\\n\")\n",
    "    \n",
    "    # List saved files\n",
    "    saved_files = os.listdir(OUTPUT_DIR)\n",
    "    print(\"üìÅ Saved files:\")\n",
    "    for file in sorted(saved_files):\n",
    "        file_path = os.path.join(OUTPUT_DIR, file)\n",
    "        if os.path.isfile(file_path):\n",
    "            size = os.path.getsize(file_path) / (1024 * 1024)\n",
    "            print(f\"  {file} ({size:.2f} MB)\")\n",
    "    \n",
    "    total_size = sum(os.path.getsize(os.path.join(OUTPUT_DIR, f)) \n",
    "                    for f in saved_files if os.path.isfile(os.path.join(OUTPUT_DIR, f)))\n",
    "    print(f\"\\nüìä Total model size: {total_size / (1024 * 1024):.2f} MB\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Model ready for download from Google Drive!\")\n",
    "    print(f\"   Location: {OUTPUT_DIR}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error saving model: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a138f1cb",
   "metadata": {},
   "source": [
    "## 12. Test the Fine-tuned Model\n",
    "\n",
    "Generate enhanced prompts for all three target models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f17c2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß™ Testing fine-tuned model...\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def test_enhancement(raw_prompt, target_model=\"ChatGPT\"):\n",
    "    \"\"\"\n",
    "    Test prompt enhancement with error handling.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        test_instruction = f\"\"\"<s>[INST] You are Prometheus, an AI assistant specialized in enhancing prompts for {target_model}.\n",
    "\n",
    "Given a user's initial prompt, enhance it following {target_model}'s best practices while preserving the original intent.\n",
    "\n",
    "User's prompt: {raw_prompt} [/INST]\n",
    "\n",
    "\"\"\"\n",
    "        \n",
    "        inputs = tokenizer(test_instruction, return_tensors=\"pt\").to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=256,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        enhanced = response.split(\"[/INST]\")[-1].strip()\n",
    "        \n",
    "        print(f\"\\nüéØ Target Model: {target_model}\")\n",
    "        print(f\"üìù Raw Prompt: {raw_prompt}\")\n",
    "        print(f\"‚ú® Enhanced Prompt:\\n{enhanced}\")\n",
    "        print(\"\\n\" + \"-\" * 80)\n",
    "        \n",
    "        return enhanced\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing {target_model}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Test examples for each model\n",
    "test_cases = [\n",
    "    (\"Write a Python function to sort a list\", \"ChatGPT\"),\n",
    "    (\"Analyze customer feedback data\", \"Claude\"),\n",
    "    (\"Create a meeting agenda\", \"Gemini\")\n",
    "]\n",
    "\n",
    "print(\"\\nüî¨ Running test cases...\\n\")\n",
    "\n",
    "for raw_prompt, target_model in test_cases:\n",
    "    test_enhancement(raw_prompt, target_model)\n",
    "\n",
    "print(\"\\n‚úÖ Testing complete!\\n\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae93752a",
   "metadata": {},
   "source": [
    "## 13. Evaluation Metrics\n",
    "\n",
    "Analyze training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa1e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üìä Analyzing training metrics...\\n\")\n",
    "\n",
    "try:\n",
    "    # Get training history\n",
    "    metrics = trainer.state.log_history\n",
    "    \n",
    "    # Extract losses\n",
    "    train_losses = [m['loss'] for m in metrics if 'loss' in m]\n",
    "    eval_losses = [m['eval_loss'] for m in metrics if 'eval_loss' in m]\n",
    "    \n",
    "    if not train_losses:\n",
    "        print(\"‚ö†Ô∏è  No training metrics found\")\n",
    "    else:\n",
    "        print(\"‚úÖ Training Metrics Summary:\\n\")\n",
    "        print(f\"  Initial training loss: {train_losses[0]:.4f}\")\n",
    "        print(f\"  Final training loss: {train_losses[-1]:.4f}\")\n",
    "        print(f\"  Loss reduction: {train_losses[0] - train_losses[-1]:.4f}\")\n",
    "        print(f\"  Improvement: {(1 - train_losses[-1]/train_losses[0])*100:.1f}%\")\n",
    "        \n",
    "        if eval_losses:\n",
    "            print(f\"\\n  Initial validation loss: {eval_losses[0]:.4f}\")\n",
    "            print(f\"  Final validation loss: {eval_losses[-1]:.4f}\")\n",
    "            print(f\"  Best validation loss: {min(eval_losses):.4f}\")\n",
    "            \n",
    "            # Check for overfitting\n",
    "            gap = eval_losses[-1] - train_losses[-1]\n",
    "            print(f\"\\n  Train-Val gap: {gap:.4f}\")\n",
    "            if gap > 0.5:\n",
    "                print(f\"  ‚ö†Ô∏è  Large gap suggests possible overfitting\")\n",
    "            else:\n",
    "                print(f\"  ‚úÖ Train-Val gap is acceptable\")\n",
    "        \n",
    "        print(f\"\\n  Total training steps: {len(train_losses)}\")\n",
    "    \n",
    "    # Plot losses if matplotlib available\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        if train_losses:\n",
    "            plt.figure(figsize=(12, 5))\n",
    "            \n",
    "            plt.subplot(1, 2, 1)\n",
    "            plt.plot(train_losses, label='Training Loss', alpha=0.7, linewidth=2)\n",
    "            plt.xlabel('Steps')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Training Loss Over Time')\n",
    "            plt.legend()\n",
    "            plt.grid(alpha=0.3)\n",
    "            \n",
    "            if eval_losses:\n",
    "                plt.subplot(1, 2, 2)\n",
    "                eval_steps = [i * (len(train_losses) // len(eval_losses)) for i in range(len(eval_losses))]\n",
    "                plt.plot(train_losses, label='Training Loss', alpha=0.5, linewidth=1)\n",
    "                plt.plot(eval_steps, eval_losses, label='Validation Loss', \n",
    "                        marker='o', linewidth=2, markersize=6)\n",
    "                plt.xlabel('Steps')\n",
    "                plt.ylabel('Loss')\n",
    "                plt.title('Training vs Validation Loss')\n",
    "                plt.legend()\n",
    "                plt.grid(alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(\"\\n‚úÖ Loss curves plotted above\")\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"\\nüí° Install matplotlib to visualize loss curves:\")\n",
    "        print(\"   !pip install matplotlib\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error analyzing metrics: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a40e20",
   "metadata": {},
   "source": [
    "## 14. Next Steps - Integration Guide\n",
    "\n",
    "### üì• Download Model from Google Drive\n",
    "\n",
    "1. **Navigate to Google Drive**\n",
    "   - Open https://drive.google.com\n",
    "   - Go to `MyDrive/Prometheus/models/prometheus-lora/`\n",
    "\n",
    "2. **Download all files** (~80-150 MB total)\n",
    "   - `adapter_model.bin`\n",
    "   - `adapter_config.json`\n",
    "   - `tokenizer.json`\n",
    "   - `tokenizer_config.json`\n",
    "   - `special_tokens_map.json`\n",
    "\n",
    "3. **Move to local project**\n",
    "   ```bash\n",
    "   cd /run/media/kabe/Kabe_s\\ Personal/Projects/Prometheus\n",
    "   mkdir -p models/prometheus-lora\n",
    "   # Move downloaded files to models/prometheus-lora/\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Backend Integration\n",
    "\n",
    "**1. Create model inference module:**\n",
    "\n",
    "```bash\n",
    "mkdir -p backend/app/model\n",
    "touch backend/app/model/__init__.py\n",
    "touch backend/app/model/inference.py\n",
    "```\n",
    "\n",
    "**2. Add dependencies to `backend/requirements.txt`:**\n",
    "\n",
    "```\n",
    "transformers>=4.41.0\n",
    "peft>=0.11.0\n",
    "bitsandbytes>=0.43.0\n",
    "accelerate>=0.30.0\n",
    "torch>=2.0.0\n",
    "```\n",
    "\n",
    "**3. Update `backend/app/main.py` to load model on startup**\n",
    "\n",
    "**4. Implement `/augment` endpoint with model inference**\n",
    "\n",
    "---\n",
    "\n",
    "### üöÄ Deployment Options\n",
    "\n",
    "**Option A: Local GPU (Development)**\n",
    "- Requires NVIDIA GPU with 8GB+ VRAM\n",
    "- Fast iteration, free\n",
    "\n",
    "**Option B: Cloud GPU (Production)**\n",
    "- AWS SageMaker, Google Cloud AI Platform, Azure ML\n",
    "- Auto-scaling, managed infrastructure\n",
    "- Cost: ~$0.50-2.00 per hour (depending on GPU)\n",
    "\n",
    "**Option C: CPU-only (Low traffic)**\n",
    "- Slower inference (~10-20 seconds per request)\n",
    "- No special hardware needed\n",
    "- Fine for prototyping\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Success Criteria\n",
    "\n",
    "Your fine-tuned model is ready when:\n",
    "- ‚úÖ Training loss decreased below 1.5\n",
    "- ‚úÖ Validation loss decreased and stayed close to training loss\n",
    "- ‚úÖ Test outputs show model-specific styles (ChatGPT: conversational, Claude: XML tags, Gemini: concise)\n",
    "- ‚úÖ Enhanced prompts are significantly more detailed than inputs\n",
    "- ‚úÖ Model files saved successfully to Google Drive\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Performance Tips\n",
    "\n",
    "**For faster inference:**\n",
    "1. Merge LoRA weights into base model (optional)\n",
    "2. Use `torch.compile()` for 2x speedup (PyTorch 2.0+)\n",
    "3. Implement batching for multiple requests\n",
    "4. Cache tokenizer and model on server startup\n",
    "\n",
    "**For better quality:**\n",
    "1. Generate with temperature=0.7-0.9 for variety\n",
    "2. Use top_p=0.9 for nucleus sampling\n",
    "3. Set max_new_tokens=256-512 depending on needed length\n",
    "4. Generate multiple variations and let user choose\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Congratulations!\n",
    "\n",
    "You've successfully fine-tuned Prometheus! The model can now:\n",
    "- Enhance prompts with ChatGPT's conversational style\n",
    "- Structure prompts with Claude's XML formatting\n",
    "- Create concise, actionable prompts for Gemini\n",
    "\n",
    "**Next:** Integrate into backend and test with real user queries!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f189810",
   "metadata": {},
   "source": [
    "# Prometheus Prompt Enhancement - LoRA Fine-tuning\n",
    "\n",
    "This notebook fine-tunes a language model using LoRA (Low-Rank Adaptation) on the Prometheus training dataset to enhance prompts with model-specific styles (ChatGPT, Claude, Gemini).\n",
    "\n",
    "**Dataset**: 1,000 training examples with model-specific enhancements  \n",
    "**Base Model**: Mistral-7B-Instruct-v0.1  \n",
    "**Method**: QLoRA (4-bit quantization + LoRA)  \n",
    "**Task**: Text-to-text prompt enhancement\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6efb85",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Install required libraries for QLoRA fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150cad1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers==4.36.0 peft==0.7.1 bitsandbytes==0.41.3 accelerate==0.25.0 datasets==2.15.0 trl==0.7.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f369ab",
   "metadata": {},
   "source": [
    "## 2. Mount Google Drive & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f1b8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "import os\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65fbdce1",
   "metadata": {},
   "source": [
    "## 3. Configuration\n",
    "\n",
    "Set paths and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698318de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths (adjust these to your Google Drive structure)\n",
    "DATASET_PATH = \"/content/drive/MyDrive/Prometheus/training_data/training_dataset.jsonl\"\n",
    "OUTPUT_DIR = \"/content/drive/MyDrive/Prometheus/models/prometheus-lora\"\n",
    "CHECKPOINT_DIR = \"/content/drive/MyDrive/Prometheus/checkpoints\"\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "\n",
    "# LoRA configuration\n",
    "LORA_R = 16  # Rank\n",
    "LORA_ALPHA = 32  # Scaling factor\n",
    "LORA_DROPOUT = 0.05\n",
    "TARGET_MODULES = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# Training hyperparameters\n",
    "BATCH_SIZE = 4\n",
    "GRADIENT_ACCUMULATION_STEPS = 4  # Effective batch size = 16\n",
    "LEARNING_RATE = 2e-4\n",
    "NUM_EPOCHS = 3\n",
    "MAX_SEQ_LENGTH = 512\n",
    "WARMUP_STEPS = 100\n",
    "\n",
    "print(\"Configuration loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2aaaefa",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Dataset\n",
    "\n",
    "Load the Prometheus training dataset and format it for instruction fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69657c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataset = load_dataset('json', data_files=DATASET_PATH, split='train')\n",
    "\n",
    "# Split into train/validation (90/10)\n",
    "dataset = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = dataset['train']\n",
    "eval_dataset = dataset['test']\n",
    "\n",
    "print(f\"Training examples: {len(train_dataset)}\")\n",
    "print(f\"Validation examples: {len(eval_dataset)}\")\n",
    "print(f\"\\nSample entry:\")\n",
    "print(json.dumps(train_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a53527",
   "metadata": {},
   "source": [
    "## 5. Format Data for Instruction Tuning\n",
    "\n",
    "Create instruction-following format: system prompt + user input ‚Üí model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdb70f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_instruction(example):\n",
    "    \"\"\"Format training example as instruction-following conversation.\"\"\"\n",
    "    target_model = example['target_model']\n",
    "    raw_prompt = example['raw_prompt']\n",
    "    enhanced_prompt = example['enhanced_prompt']\n",
    "    \n",
    "    # Instruction template\n",
    "    instruction = f\"\"\"<s>[INST] You are Prometheus, an AI assistant specialized in enhancing prompts for {target_model}.\n",
    "\n",
    "Given a user's initial prompt, enhance it following {target_model}'s best practices while preserving the original intent.\n",
    "\n",
    "User's prompt: {raw_prompt} [/INST]\n",
    "\n",
    "{enhanced_prompt}</s>\"\"\"\n",
    "    \n",
    "    return {\"text\": instruction}\n",
    "\n",
    "# Apply formatting\n",
    "train_dataset = train_dataset.map(format_instruction)\n",
    "eval_dataset = eval_dataset.map(format_instruction)\n",
    "\n",
    "print(\"Sample formatted instruction:\")\n",
    "print(train_dataset[0]['text'][:500] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265ee8ff",
   "metadata": {},
   "source": [
    "## 6. Load Model with 8-bit Quantization\n",
    "\n",
    "Load Mistral-7B with 8-bit quantization for stable, memory-efficient training. 8-bit quantization is more stable than 4-bit and requires ~7GB VRAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5987b651",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ü§ñ Loading Mistral-7B model with 8-bit quantization...\")\n",
    "print(\"   (8-bit is more stable than 4-bit)\\n\")\n",
    "print(\"‚è≥ This will take 2-3 minutes to download (~4.5GB)...\\n\")\n",
    "\n",
    "try:\n",
    "    # Configure 8-bit quantization (more stable than 4-bit)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_threshold=6.0,\n",
    "        llm_int8_has_fp16_weight=False\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ 8-bit quantization config created\\n\")\n",
    "    \n",
    "    # Load base model\n",
    "    print(\"üì• Downloading model from HuggingFace...\\n\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        use_cache=False  # Required for gradient checkpointing\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úÖ Model loaded successfully!\\n\")\n",
    "    \n",
    "    # Verify model is loaded correctly\n",
    "    print(f\"üìã Model type: {type(model).__name__}\")\n",
    "    assert not isinstance(model, str), \"ERROR: Model is a string, not a model object!\"\n",
    "    \n",
    "    # Enable gradient checkpointing to save memory\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"‚úÖ Gradient checkpointing enabled\\n\")\n",
    "    \n",
    "    # Prepare for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    print(\"‚úÖ Model prepared for LoRA training\\n\")\n",
    "    \n",
    "    # Show model info\n",
    "    print(\"üìä Model Information:\")\n",
    "    print(f\"  Memory footprint: {model.get_memory_footprint() / 1e9:.2f} GB\")\n",
    "    print(f\"  Device map: {model.hf_device_map}\")\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"  Total parameters: {total_params:,}\")\n",
    "    \n",
    "    # Check GPU memory\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated(0) / 1e9\n",
    "        reserved = torch.cuda.memory_reserved(0) / 1e9\n",
    "        total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "        \n",
    "        print(f\"\\nüíæ GPU Memory:\")\n",
    "        print(f\"  Allocated: {allocated:.2f} GB\")\n",
    "        print(f\"  Reserved: {reserved:.2f} GB\")\n",
    "        print(f\"  Total: {total:.2f} GB\")\n",
    "        print(f\"  Free: {total - reserved:.2f} GB\")\n",
    "        \n",
    "        if reserved > total * 0.85:\n",
    "            print(f\"\\n‚ö†Ô∏è  High memory usage ({reserved/total*100:.1f}%)\")\n",
    "            print(f\"   Consider reducing BATCH_SIZE to 2\")\n",
    "        else:\n",
    "            print(f\"\\n‚úÖ Sufficient memory available for training\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"‚úÖ MODEL READY - Proceed to Cell 7 (Configure LoRA)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    print(\"\\n‚ùå GPU Out of Memory!\")\n",
    "    print(\"\\nüîß Solutions:\")\n",
    "    print(\"   1. Restart runtime to clear GPU memory\")\n",
    "    print(\"   2. Reduce BATCH_SIZE to 2 in Cell 3\")\n",
    "    print(\"   3. Reduce MAX_SEQ_LENGTH to 384 in Cell 3\")\n",
    "    sys.exit(1)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error loading model: {e}\")\n",
    "    print(f\"   Error type: {type(e).__name__}\")\n",
    "    \n",
    "    # Show debug info\n",
    "    print(f\"\\nüîç Debug info:\")\n",
    "    try:\n",
    "        print(f\"   model variable type: {type(model)}\")\n",
    "        if isinstance(model, str):\n",
    "            print(f\"   ERROR: model is still a string: {model}\")\n",
    "            print(f\"   This means the model didn't load properly\")\n",
    "    except:\n",
    "        print(f\"   model variable not defined\")\n",
    "    \n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c6062f",
   "metadata": {},
   "source": [
    "## 7. Configure LoRA Adapters\n",
    "\n",
    "Add LoRA adapters to the model for parameter-efficient fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b69d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    target_modules=TARGET_MODULES,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1666bf2e",
   "metadata": {},
   "source": [
    "## 8. Load Tokenizer and Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15f84b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    outputs = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LENGTH,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    outputs[\"labels\"] = outputs[\"input_ids\"].copy()\n",
    "    return outputs\n",
    "\n",
    "# Tokenize datasets\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenization complete!\")\n",
    "print(f\"Training tokens: {len(tokenized_train)}\")\n",
    "print(f\"Validation tokens: {len(tokenized_eval)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafe6cf6",
   "metadata": {},
   "source": [
    "## 9. Configure Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e99ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=CHECKPOINT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    logging_steps=10,\n",
    "    eval_steps=50,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    fp16=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=3,\n",
    "    push_to_hub=False\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Total epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Effective batch size: {BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Total training steps: ~{len(tokenized_train) // (BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS) * NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887383a7",
   "metadata": {},
   "source": [
    "## 10. Train the Model\n",
    "\n",
    "Start LoRA fine-tuning. This will take approximately 1-2 hours on a T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401fbb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval\n",
    ")\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb6288",
   "metadata": {},
   "source": [
    "## 11. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fc3454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters\n",
    "model.save_pretrained(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "\n",
    "print(f\"‚úÖ Model saved to: {OUTPUT_DIR}\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(os.listdir(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac311987",
   "metadata": {},
   "source": [
    "## 12. Test the Fine-tuned Model\n",
    "\n",
    "Generate enhanced prompts using the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c33ba88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_enhancement(raw_prompt, target_model=\"ChatGPT\"):\n",
    "    \"\"\"Test prompt enhancement with the fine-tuned model.\"\"\"\n",
    "    test_instruction = f\"\"\"<s>[INST] You are Prometheus, an AI assistant specialized in enhancing prompts for {target_model}.\n",
    "\n",
    "Given a user's initial prompt, enhance it following {target_model}'s best practices while preserving the original intent.\n",
    "\n",
    "User's prompt: {raw_prompt} [/INST]\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(test_instruction, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=256,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    enhanced = response.split(\"[/INST]\")[-1].strip()\n",
    "    \n",
    "    print(f\"Target Model: {target_model}\")\n",
    "    print(f\"Raw Prompt: {raw_prompt}\")\n",
    "    print(f\"Enhanced Prompt:\\n{enhanced}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    return enhanced\n",
    "\n",
    "# Test examples\n",
    "test_enhancement(\"Write a Python function to sort a list\", \"ChatGPT\")\n",
    "test_enhancement(\"Analyze customer feedback data\", \"Claude\")\n",
    "test_enhancement(\"Create a meeting agenda\", \"Gemini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be273c",
   "metadata": {},
   "source": [
    "## 13. Evaluation Metrics\n",
    "\n",
    "Check training and validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa2bff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training metrics\n",
    "metrics = trainer.state.log_history\n",
    "\n",
    "# Extract losses\n",
    "train_losses = [m['loss'] for m in metrics if 'loss' in m]\n",
    "eval_losses = [m['eval_loss'] for m in metrics if 'eval_loss' in m]\n",
    "\n",
    "print(\"Training Metrics Summary:\")\n",
    "print(f\"  Final training loss: {train_losses[-1]:.4f}\")\n",
    "print(f\"  Final validation loss: {eval_losses[-1]:.4f}\")\n",
    "print(f\"  Best validation loss: {min(eval_losses):.4f}\")\n",
    "print(f\"  Total training steps: {len(train_losses)}\")\n",
    "\n",
    "# Optional: Plot losses if matplotlib is available\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Training Loss', alpha=0.7)\n",
    "    plt.plot(range(0, len(train_losses), len(train_losses)//len(eval_losses)), \n",
    "             eval_losses, label='Validation Loss', marker='o')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Prometheus LoRA Fine-tuning - Loss Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"\\nInstall matplotlib to visualize loss curves: !pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ee45da",
   "metadata": {},
   "source": [
    "## 14. Next Steps\n",
    "\n",
    "**To use this model in the Prometheus backend:**\n",
    "\n",
    "1. **Export the LoRA adapters** from Google Drive to your local machine\n",
    "2. **Update `backend/app/main.py`** to load the fine-tuned model:\n",
    "   ```python\n",
    "   from peft import PeftModel, PeftConfig\n",
    "   \n",
    "   config = PeftConfig.from_pretrained(\"path/to/prometheus-lora\")\n",
    "   base_model = AutoModelForCausalLM.from_pretrained(config.base_model_name_or_path)\n",
    "   model = PeftModel.from_pretrained(base_model, \"path/to/prometheus-lora\")\n",
    "   ```\n",
    "3. **Implement the `/augment` endpoint** to use this model for generation\n",
    "4. **Optional:** Merge LoRA weights into base model for faster inference:\n",
    "   ```python\n",
    "   merged_model = model.merge_and_unload()\n",
    "   merged_model.save_pretrained(\"prometheus-merged\")\n",
    "   ```\n",
    "\n",
    "**Performance tips:**\n",
    "- For production, consider hosting on GPU-enabled cloud (AWS/GCP/Azure)\n",
    "- Use `torch.compile()` for faster inference (PyTorch 2.0+)\n",
    "- Implement batching for multiple enhancement requests\n",
    "- Monitor response quality and iterate on training data if needed\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ **Fine-tuning complete! Model ready for integration into Prometheus backend.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
